{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "network defined\n",
      "deep_network created\n",
      "NpS: [4]\n",
      "Strides: [2]\n",
      "Distances: [4]\n",
      "Lateral Distances: [0]\n",
      "gamma : 0\n",
      "tanh_factor : 1\n",
      "\n",
      "Epoch: 1\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.005\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0033333333333333335\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0025\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.002\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0016666666666666668\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0014285714285714286\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.00125\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0011111111111111111\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.001\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0009090909090909091\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0008333333333333334\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0007692307692307692\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0007142857142857143\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0006666666666666666\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.000625\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0005882352941176471\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0005555555555555556\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0005263157894736842\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0005\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Conversion: 1.0\n",
      "Current Learning Rate: 0.0004761904761904762\n",
      "\n",
      "DONE\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "train_score\n",
      "0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant SVC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9408\n",
      "test_score\n",
      "0.9337\n",
      "train_error\n",
      "0.05920000000000003\n",
      "test_error\n",
      "0.06630000000000003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "using Knet, LinearAlgebra, Random, SparseArrays, Statistics\n",
    "\n",
    "mutable struct network_weights\n",
    "    distance_parameter;\n",
    "    input_dim;\n",
    "    stride;\n",
    "    NpS;\n",
    "    output_dim;\n",
    "    W;\n",
    "    L;\n",
    "    W_structure;\n",
    "    L_structure;\n",
    "    previous_NpS;\n",
    "    lateral_distance;\n",
    "end\n",
    "    \n",
    "\n",
    "function create_h_distances(n::network_weights)\n",
    "    distances = zeros(n.output_dim^2, n.input_dim, n.input_dim)\n",
    "    dict_input_2_position = Dict()\n",
    "    for row_index in 1:n.input_dim\n",
    "        for column_index in 1:n.input_dim\n",
    "            input_index = row_index*n.input_dim + column_index\n",
    "            dict_input_2_position[row_index, column_index] = input_index\n",
    "    \n",
    "        end\n",
    "    end\n",
    "    centers = []\n",
    "    dict_output_2_position = Dict()\n",
    "    for i in 1:n.output_dim\n",
    "        for j in 1:n.output_dim\n",
    "            stride_padding = n.stride/2\n",
    "            neuron_center = [i*n.stride + stride_padding, j*n.stride + stride_padding]\n",
    "            push!(centers,neuron_center)\n",
    "            neuron_index = (i-1)*n.output_dim + j\n",
    "            dict_output_2_position[neuron_index] = neuron_center\n",
    "            for k in 1:n.input_dim\n",
    "                 for l in 1:n.input_dim\n",
    "                    #size issue here neuron index becomes 50 where it should be 49\n",
    "                    distances[neuron_index, k,l] = norm([k+0.5,l+0.5]-neuron_center)\n",
    "                 end\n",
    "           end\n",
    "        end\n",
    "    end\n",
    "    above_threshold = distances .> n.distance_parameter\n",
    "    below_threshold = distances .<= n.distance_parameter\n",
    "    distances[above_threshold] .= 0\n",
    "    distances[below_threshold] .= 1\n",
    "    distances = reshape(distances,(n.output_dim^2, n.input_dim^2))\n",
    "    return distances\n",
    "end\n",
    "\n",
    "function create_ah_distances(n::network_weights)\n",
    "    centers = []\n",
    "    dict_output_2_position = Dict()\n",
    "    for i in 1:n.output_dim\n",
    "        for j in 1:n.output_dim\n",
    "            stride_padding = n.stride/2\n",
    "            neuron_center = [(i-1)*n.stride + stride_padding, (j-1)*n.stride + stride_padding]\n",
    "            push!(centers,neuron_center)\n",
    "            neuron_index = (i-1)*n.output_dim + j\n",
    "            dict_output_2_position[neuron_index] = neuron_center\n",
    "        end\n",
    "    end\n",
    "    distances_ah = zeros(n.output_dim^2, n.output_dim^2)\n",
    "    for row_index in keys(dict_output_2_position)\n",
    "        center = dict_output_2_position[row_index]\n",
    "        for column_index in keys(dict_output_2_position)\n",
    "            other_center = dict_output_2_position[column_index]\n",
    "            distances_ah[row_index, column_index] = norm(other_center - center)\n",
    "        end\n",
    "    end\n",
    "    above_threshold = distances_ah .> n.lateral_distance #*self.anti_hebbian_binary\n",
    "    below_threshold = distances_ah .<= n.lateral_distance #*self.anti_hebbian_binary\n",
    "    distances_ah[above_threshold] .= 0\n",
    "    distances_ah[below_threshold] .= 1\n",
    "    return distances_ah\n",
    "end    \n",
    "\n",
    "function create_L(n::network_weights)\n",
    "    #mat = n.create_ah_distances()\n",
    "    mat = create_ah_distances(n)\n",
    "    #blocks = [[mat]*n.NpS]*n.NpS\n",
    "    L_mat = repeat(mat,n.NpS,n.NpS) ## !! Figure this one out\n",
    "    return L_mat\n",
    "end\n",
    "\n",
    "function create_W(n::network_weights)\n",
    "    #mat = n.create_ah_distances()\n",
    "    mat = create_h_distances(n)\n",
    "    #println(\"size of mat in create_h_distances\")\n",
    "    #println(size(mat))\n",
    "    #blocks = [[mat]*n.previous_NpS]*n.NpS\n",
    "    #println(\"n.previous_NpS\")\n",
    "    #println(n.previous_NpS)\n",
    "    W_mat = repeat(mat,n.NpS,n.previous_NpS) ## !! Figure this one out\n",
    "    return W_mat\n",
    "end\n",
    "    \n",
    "    \n",
    "function create_weights_matrix(n::network_weights)\n",
    "    n.W_structure = create_W(n)\n",
    "    n.L_structure = create_L(n)\n",
    "    factor = sqrt(((sum(n.W_structure)/n.NpS)/(n.output_dim^2)))\n",
    "    n.W = n.W_structure.*randn(size(n.W_structure))/factor\n",
    "    n.L = n.L_structure.*Matrix{Float64}(I,n.NpS * n.output_dim^2,n.NpS * n.output_dim^2)\n",
    "end\n",
    "\n",
    "function activation_function(vec)\n",
    "    tanh.(vec)\n",
    "end\n",
    "\n",
    "println(gpu()>=0)\n",
    "if gpu() >= 0  # gpu() returns a device id >= 0 if there is a GPU, -1 otherwise \n",
    "    atype = KnetArray{Float32}  # KnetArrays are stored and operated in the GPU\n",
    "end\n",
    "\n",
    "function convert_to_GPU(vec)\n",
    "    vec=convert(atype,vec)\n",
    "end\n",
    "\n",
    "mutable struct deep_network_GPU\n",
    "    image_dim;\n",
    "    channels;\n",
    "    NpSs;\n",
    "    strides;\n",
    "    distances;\n",
    "    lateral_distances;\n",
    "    layers;\n",
    "    gamma;\n",
    "    lr;\n",
    "    lr_floor;\n",
    "    current_lr;\n",
    "    decay;\n",
    "    conversion_tickers;\n",
    "    costs; \n",
    "    epoch;\n",
    "    structure;\n",
    "    deep_matrix_weights;\n",
    "    deep_matrix_structure;\n",
    "    deep_matrix_identity;\n",
    "    weights_adjustment_matrix;\n",
    "    weights_update_matrix;\n",
    "    grad_matrix;\n",
    "    n_images;\n",
    "    dict_weights;\n",
    "    dimensions; \n",
    "    g_vec;\n",
    "    mult_vec;\n",
    "    euler_step;\n",
    "    tanh_factors;\n",
    "    mult_factors;\n",
    "    W_gpu;\n",
    "end\n",
    "\n",
    "\n",
    "function int(x)\n",
    "convert(Int,floor(x))\n",
    "end\n",
    "\n",
    "function create_deep_network(dn::deep_network_GPU)\n",
    "    for i in 1:dn.layers+1\n",
    "        if i==1\n",
    "            dim=1\n",
    "        else\n",
    "            dim = int(prod(dn.strides[1:i-1]))\n",
    "        end\n",
    "        push!(dn.dimensions,(int((dn.image_dim/dim)^2)*hcat([dn.channels],dn.NpSs)[i]))\n",
    "    end\n",
    "\n",
    "    for i in 1:dn.layers\n",
    "        if i==1\n",
    "            dim=1\n",
    "        else\n",
    "            dim = int(prod(dn.strides[1:i-1]))\n",
    "        end\n",
    "        layer_input_dim = int(dn.image_dim/dim)\n",
    "\n",
    "         dn.dict_weights[i]=network_weights(dn.distances[i],layer_input_dim,dn.strides[i],hcat([dn.channels],dn.NpSs)[i+1],\n",
    "         int(layer_input_dim/dn.strides[i]),[],[],\n",
    "         [], [], hcat([dn.channels],dn.NpSs)[i], dn.lateral_distances[i])\n",
    "         create_weights_matrix(dn.dict_weights[i])\n",
    "\n",
    "    end\n",
    "\n",
    "    matrix_block = []\n",
    "    structure_block = []\n",
    "    matrix_identity = []\n",
    "    weight_adjustment_block = []\n",
    "    gradient_update_block = []\n",
    "    abs_structure_block = []\n",
    "    for (i, ele_row) in enumerate(dn.dimensions)\n",
    "        #println(\"inside the loop\")\n",
    "        #println(i)\n",
    "        #println(ele_row)\n",
    "        row_block = []\n",
    "        struc_block = []\n",
    "        row_identity_block = []\n",
    "        weights_adj_block = []\n",
    "        grad_update_block = []\n",
    "        abs_struc_block = []\n",
    "        start_block = max(i-1, 1)\n",
    "\n",
    "        end_block = max(length(dn.dimensions)-start_block-2, 1)\n",
    "        if i == 1\n",
    "            #println(\"pushed 1 element to row_block\")\n",
    "            push!(row_block,zeros(ele_row, sum(dn.dimensions)))\n",
    "            push!(struc_block,zeros(ele_row, sum(dn.dimensions)))\n",
    "            push!(row_identity_block, zeros(ele_row, sum(dn.dimensions)))\n",
    "            push!(weights_adj_block, zeros(ele_row, sum(dn.dimensions)))\n",
    "            push!(grad_update_block, zeros((ele_row, sum(dn.dimensions))))\n",
    "            push!(abs_struc_block, zeros((ele_row, sum(dn.dimensions))))\n",
    "        elseif i == length(dn.dimensions)\n",
    "            #i=2 here\n",
    "            if start_block > 1\n",
    "                push!(row_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                push!(struc_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                push!(row_identity_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                push!(weights_adj_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                push!(grad_update_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                push!(abs_struc_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
    "                #println(\"second elseif first if\")\n",
    "                #println(size(zeros(ele_row, int(sum(dn.dimensions[1:start_block])))))\n",
    "            end\n",
    "            #println(\"pushed 1 element to row_block\")\n",
    "            push!(row_block,dn.dict_weights[i-1].W)\n",
    "            #println(\"pushed 1 element to row_block\")\n",
    "            push!(row_block,dn.dict_weights[i-1].L)\n",
    "            push!(struc_block,dn.dict_weights[i-1].W_structure/dn.mult_factors[i-1])\n",
    "            push!(struc_block,-dn.dict_weights[i-1].L_structure)\n",
    "\n",
    "            push!(abs_struc_block,dn.dict_weights[i-1].W_structure)\n",
    "            push!(abs_struc_block,dn.dict_weights[i-1].L_structure)\n",
    "\n",
    "            push!(row_identity_block,zeros(size(dn.dict_weights[i-1].W_structure)))\n",
    "            push!(row_identity_block,Matrix{Float64}(I,size(dn.dict_weights[i-1].L_structure,1),size(dn.dict_weights[i-1].L_structure,1))) #identity matrix\n",
    "\n",
    "            push!(weights_adj_block,dn.dict_weights[i-1].W_structure)\n",
    "            push!(weights_adj_block,dn.dict_weights[i-1].L_structure)\n",
    "\n",
    "            push!(grad_update_block,dn.dict_weights[i-1].W_structure)\n",
    "            push!(grad_update_block,dn.dict_weights[i-1].L_structure/2)\n",
    "\n",
    "        end\n",
    "        # we only push row_block to matrix_block. Check row_block\n",
    "        push!(matrix_block,row_block)\n",
    "        push!(structure_block,struc_block)\n",
    "        push!(matrix_identity,row_identity_block)\n",
    "        push!(weight_adjustment_block,weights_adj_block)\n",
    "        push!(gradient_update_block,grad_update_block)\n",
    "        push!(abs_structure_block,abs_struc_block)\n",
    "    end\n",
    "\n",
    "\n",
    "    dn.deep_matrix_weights = convert_to_GPU(vcat(matrix_block[1][1],hcat(matrix_block[2][1],matrix_block[2][2]))) \n",
    "    dn.deep_matrix_structure = convert_to_GPU(vcat(structure_block[1][1],hcat(structure_block[2][1],structure_block[2][2])))\n",
    "    dn.deep_matrix_identity = convert_to_GPU(vcat(matrix_identity[1][1],hcat(matrix_identity[2][1],matrix_identity[2][2])))\n",
    "    dn.weights_adjustment_matrix = convert_to_GPU(vcat(weight_adjustment_block[1][1],hcat(weight_adjustment_block[2][1],weight_adjustment_block[2][2])))\n",
    "    dn.weights_update_matrix = convert_to_GPU(vcat(gradient_update_block[1][1],hcat(gradient_update_block[2][1],gradient_update_block[2][2])))\n",
    "    dn.structure = convert_to_GPU(vcat(abs_structure_block[1][1],hcat(abs_structure_block[2][1],abs_structure_block[2][2])))\n",
    "\n",
    "    println(\"deep_network created\")\n",
    "end\n",
    "\n",
    "# First define the neural dynamics\n",
    "function neural_dynamics(dn::deep_network_GPU,img)\n",
    "    conversion_ticker = 0\n",
    "    x = img\n",
    "    u_vec = convert_to_GPU(zeros(sum(dn.dimensions)))\n",
    "    #representation vector\n",
    "    r_vec = convert_to_GPU(zeros(sum(dn.dimensions)))\n",
    "    \n",
    "    r_vec[1:dn.channels*dn.image_dim^2] .= x\n",
    "    delta = repeat([1e10],dn.layers) \n",
    "\n",
    "    dn.W_gpu = dn.deep_matrix_weights.*dn.deep_matrix_structure .+ dn.deep_matrix_identity\n",
    "    updates = 0\n",
    "    while updates < 1000\n",
    "        if sum(delta .< 1e-4) == length(delta)\n",
    "            conversion_ticker=1\n",
    "            break\n",
    "        end\n",
    "        lr = max((dn.euler_step/(1+0.005*updates)), 0.05)\n",
    "        delta_u = -u_vec .+ dn.W_gpu*r_vec\n",
    "        u_vec[dn.channels*(dn.image_dim)^2:end] += lr*delta_u[dn.channels*dn.image_dim^2:end]\n",
    "        r_vec[dn.channels*(dn.image_dim)^2:end] = activation_function(u_vec[dn.channels*dn.image_dim^2:end])\n",
    "        updates += 1\n",
    "        if (updates+1)%100 == 0\n",
    "            # may be problems with indexing here. made changes here that may not work under different modes.\n",
    "            #println(size(dn.dimensions[2:end]))\n",
    "            for layer in 1:dn.layers\n",
    "                start_token_large = sum(dn.dimensions[1:layer])\n",
    "                end_token_large = sum(dn.dimensions[1:layer+1])\n",
    "                start_token_small = int(sum(dn.dimensions[2:end])) \n",
    "                end_token_small = sum(dn.dimensions[2:end]) \n",
    "                delta_layer = norm(delta_u[start_token_small:end_token_small])/norm(u_vec[start_token_large:end_token_large])\n",
    "                delta[layer] = delta_layer\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return r_vec, conversion_ticker\n",
    "end\n",
    "\n",
    "\n",
    "function update_weights(dn::deep_network_GPU,r_vec)\n",
    "     dn.current_lr = max(dn.lr/(1+dn.decay*dn.epoch), dn.lr_floor)\n",
    "     #r_vec = cp.asnumpy(r_vec)\n",
    "     update_matrix = r_vec*r_vec' #update for L matrix\n",
    "     grad_weights = convert_to_GPU(dn.weights_update_matrix).*(update_matrix - convert_to_GPU(dn.weights_adjustment_matrix.*dn.deep_matrix_weights))\n",
    "     dn.deep_matrix_weights += dn.current_lr*grad_weights\n",
    "end\n",
    "\n",
    "function training(dn::deep_network_GPU, images)\n",
    "     dn.n_images = size(images,1)\n",
    "         #epoch_start = time.time()\n",
    "         sum_ticker = 0\n",
    "         for img in 1:size(images,1)\n",
    "             r, conversion_ticker = neural_dynamics(dn,images[img,:])\n",
    "             sum_ticker += conversion_ticker\n",
    "             update_weights(dn,r)\n",
    "         end\n",
    "         dn.epoch+=1\n",
    "         #epoch_end = time.time()\n",
    "         #epoch_time = epoch_end-epoch_start\n",
    "         push!(dn.conversion_tickers,sum_ticker/dn.n_images)\n",
    "         println(\"\")\n",
    "         println(\"Epoch: \"*string(dn.epoch))\n",
    "         println(\"Conversion: \"*string(dn.conversion_tickers[end]))\n",
    "         println(\"Current Learning Rate: \"*string(dn.current_lr)) \n",
    "         println(\"\")\n",
    " end        \n",
    "\n",
    "function get_mnist()\n",
    "    \n",
    "    #dtrn,dtst = mnistdata(batchsize=1;xsize=(784,:),xtype=atype) # The setting is online so batchsize is set to 1.\n",
    "    X_train, y_train, X_test, y_test = mnist() # loading the data\n",
    "    X_train = convert(atype,reshape(X_train, 784, 60000)')\n",
    "    X_test = convert(atype,reshape(X_test, 784, 10000)')\n",
    "    \n",
    "     # preprocess data\n",
    "     X_train =X_train .- mean(X_train;dims=2)\n",
    "     X_test =X_test.- mean(X_test;dims=2)\n",
    "    \n",
    "    #summary.(first(X_train))\n",
    "    return X_train, X_test, y_train, y_test\n",
    "end\n",
    "    \n",
    "\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data\n",
    "x_train, x_test, y_train, y_test = get_mnist()\n",
    "file=\"Trained_network.jld2\"\n",
    "\n",
    "tanh_factors=1\n",
    "stride=[2]\n",
    "distance_parameter=[4]\n",
    "gamma_factor=0\n",
    "mult_factor=1\n",
    "NpSs=[4]\n",
    "lateral_distance = repeat([0],length(distance_parameter))\n",
    "image_dim = 28\n",
    "channels = 1\n",
    "strides = stride\n",
    "distances = distance_parameter\n",
    "distances_lateral = lateral_distance\n",
    "tanh_factors = tanh_factors\n",
    "layers = length(distance_parameter)\n",
    "gamma = gamma_factor\n",
    "mult_factors = mult_factor\n",
    "\n",
    "lr=5e-3;\n",
    "lr_floor = 1e-4;\n",
    "decay=0.5\n",
    "conversion_tickers = []\n",
    "costs = []\n",
    "epoch =0\n",
    "structure =[]\n",
    "deep_matrix_weights= [];\n",
    "deep_matrix_structure = [];\n",
    "deep_matrix_identity= [] ;\n",
    "weights_adjustment_matrix= [];\n",
    "weights_update_matrix= [];\n",
    "grad_matrix =[];\n",
    "n_images =[];\n",
    "dict_weights = Dict()\n",
    "dimensions = [];\n",
    "g_vec=[];\n",
    "mult_vec=[];\n",
    "euler_step=0.2;\n",
    "tanh_factors=1;\n",
    "mult_factors=1;\n",
    "W_gpu=[];\n",
    "\n",
    "#if (print(\"Train from scratch? (~77s) \"); readline()[1]=='y')\n",
    "    network= deep_network_GPU(image_dim,channels,NpSs,strides,distances,lateral_distance, layers,gamma, lr,lr_floor,\n",
    "                            lr, decay,conversion_tickers, costs, epoch, structure, deep_matrix_weights,\n",
    "                            deep_matrix_structure, deep_matrix_identity, weights_adjustment_matrix,\n",
    "                            weights_update_matrix, grad_matrix, n_images, dict_weights, dimensions, g_vec,\n",
    "                            mult_vec, euler_step, tanh_factors,mult_factors , W_gpu);\n",
    "    \n",
    "    println(\"network defined\")\n",
    "\n",
    "    create_deep_network(network)\n",
    "    \n",
    "    println(\"NpS: \"*string(network.NpSs))\n",
    "    println(\"Strides: \"*string(network.strides))\n",
    "    println(\"Distances: \"*string(network.distances))\n",
    "    println(\"Lateral Distances: \"*string(network.lateral_distances))\n",
    "    println(\"gamma : \"*string(network.gamma))\n",
    "    println(\"tanh_factor : \"*string(network.tanh_factors))\n",
    "    \n",
    "    no_of_epochs=20\n",
    "    for i in 1:no_of_epochs\n",
    "        indices = 1:size(x_train,1)\n",
    "        rand_indices = rand(indices, 1000)\n",
    "        x_train_rand = x_train[rand_indices,:]\n",
    "        training(network, x_train_rand)\n",
    "    end\n",
    "    x_train_rand=nothing\n",
    "    #Knet.save(file,\"trained_network\",network)\n",
    "    #else\n",
    "    #        isfile(file)\n",
    "    #        network = Knet.load(file,\"trained_network\")\n",
    "    #end\n",
    "\n",
    "println(\"DONE\")\n",
    "\n",
    "\n",
    "train_representations=[]\n",
    "#for i in  1:int(size(x_train,1))\n",
    "N_m=10000\n",
    "for i in 1:N_m\n",
    "    train_rep, _ = neural_dynamics(network,x_train[i,:])\n",
    "     push!(train_representations, train_rep)\n",
    "     if (i+1)%1000==0\n",
    "         println(i+1)\n",
    "     end\n",
    " end\n",
    "\n",
    "x_train=nothing\n",
    "Knet.gc()\n",
    "\n",
    "for i in 1:length(train_representations)\n",
    "    train_representations[i]=convert(Array{Float64,1},train_representations[i])\n",
    "end\n",
    "\n",
    "test_representations = []\n",
    "for i in 1:size(x_test,1)\n",
    "#for i in 1:N_m\n",
    "      test_rep, _ = neural_dynamics(network, x_test[i,:])\n",
    "      push!(test_representations,test_rep)\n",
    "      if (i+1)%1000==0\n",
    "          println(i+1)\n",
    "      end\n",
    "  end\n",
    "\n",
    "x_test=nothing\n",
    "Knet.gc()\n",
    "\n",
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "classifier = SVC(max_iter=1e6, class_weight=\"balanced\",tol=1e-5,random_state=0)\n",
    "\n",
    "#fit!(classifier, train_representations, y_train)\n",
    "fit!(classifier, train_representations, y_train[1:N_m])\n",
    "\n",
    "#train_score = score(classifier, train_representations, y_train)\n",
    "train_score = score(classifier, train_representations, y_train[1:N_m])\n",
    "println(\"train_score\")\n",
    "println(train_score)\n",
    "\n",
    "for i in 1:length(test_representations)\n",
    "    test_representations[i]=convert(Array{Float64,1},test_representations[i])\n",
    "end\n",
    "\n",
    "\n",
    "test_score=score(classifier,test_representations, y_test)\n",
    "#test_score=score(classifier,test_representations, y_test[1:N_m])\n",
    "println(\"test_score\")\n",
    "println(test_score)\n",
    "println(\"train_error\")\n",
    "println(1-train_score)\n",
    "println(\"test_error\")\n",
    "println(1-test_score)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
