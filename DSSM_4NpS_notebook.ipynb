{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "julia_on_collab.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Julia 1.3",
      "language": "julia",
      "name": "julia-1.3"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PMGwZ7aFJL8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f65bd410-2d92-4bd6-8f8a-7625644e0a7e"
      },
      "source": [
        "# Installation cell\n",
        "%%shell\n",
        "if ! command -v julia 3>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.3/julia-1.3.1-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "echo 'Done'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-27 11:42:12--  https://julialang-s3.julialang.org/bin/linux/x64/1.3/julia-1.3.1-linux-x86_64.tar.gz\n",
            "Resolving julialang-s3.julialang.org (julialang-s3.julialang.org)... 151.101.2.49, 151.101.66.49, 151.101.130.49, ...\n",
            "Connecting to julialang-s3.julialang.org (julialang-s3.julialang.org)|151.101.2.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 gce internal redirect trigger\n",
            "Location: https://storage.googleapis.com/julialang2/bin/linux/x64/1.3/julia-1.3.1-linux-x86_64.tar.gz [following]\n",
            "--2020-05-27 11:42:12--  https://storage.googleapis.com/julialang2/bin/linux/x64/1.3/julia-1.3.1-linux-x86_64.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 2607:f8b0:4001:c1b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95929584 (91M) [application/x-gzip]\n",
            "Saving to: ‘/tmp/julia.tar.gz’\n",
            "\n",
            "/tmp/julia.tar.gz   100%[===================>]  91.49M  29.7MB/s    in 3.1s    \n",
            "\n",
            "2020-05-27 11:42:15 (29.7 MB/s) - ‘/tmp/julia.tar.gz’ saved [95929584/95929584]\n",
            "\n",
            "   Cloning default registries into `~/.julia`\n",
            "   Cloning registry from \"https://github.com/JuliaRegistries/General.git\"\n",
            "\u001b[2K\u001b[?25h     Added registry `General` to `~/.julia/registries/General`\n",
            " Resolving package versions...\n",
            " Installed MbedTLS_jll ───── v2.16.0+2\n",
            " Installed SoftGlobalScope ─ v1.0.10\n",
            " Installed Parsers ───────── v1.0.4\n",
            " Installed VersionParsing ── v1.2.0\n",
            " Installed IJulia ────────── v1.21.2\n",
            " Installed MbedTLS ───────── v1.0.2\n",
            " Installed ZMQ ───────────── v1.2.0\n",
            " Installed Conda ─────────── v1.4.1\n",
            " Installed ZeroMQ_jll ────── v4.3.2+3\n",
            " Installed JSON ──────────── v0.21.0\n",
            "  Updating `~/.julia/environments/v1.3/Project.toml`\n",
            "  [7073ff75] + IJulia v1.21.2\n",
            "  Updating `~/.julia/environments/v1.3/Manifest.toml`\n",
            "  [8f4d0f93] + Conda v1.4.1\n",
            "  [7073ff75] + IJulia v1.21.2\n",
            "  [682c06a0] + JSON v0.21.0\n",
            "  [739be429] + MbedTLS v1.0.2\n",
            "  [c8ffd9c3] + MbedTLS_jll v2.16.0+2\n",
            "  [69de0a69] + Parsers v1.0.4\n",
            "  [b85f4697] + SoftGlobalScope v1.0.10\n",
            "  [81def892] + VersionParsing v1.2.0\n",
            "  [c2297ded] + ZMQ v1.2.0\n",
            "  [8f1865be] + ZeroMQ_jll v4.3.2+3\n",
            "  [2a0f44e3] + Base64 \n",
            "  [ade2ca70] + Dates \n",
            "  [8ba89e20] + Distributed \n",
            "  [7b1f6079] + FileWatching \n",
            "  [b77e0a4c] + InteractiveUtils \n",
            "  [76f85450] + LibGit2 \n",
            "  [8f399da3] + Libdl \n",
            "  [56ddb016] + Logging \n",
            "  [d6f4376e] + Markdown \n",
            "  [a63ad114] + Mmap \n",
            "  [44cfe95a] + Pkg \n",
            "  [de0858da] + Printf \n",
            "  [3fa0cd96] + REPL \n",
            "  [9a3f8284] + Random \n",
            "  [ea8e919c] + SHA \n",
            "  [9e88b42a] + Serialization \n",
            "  [6462fe0b] + Sockets \n",
            "  [8dfed614] + Test \n",
            "  [cf7118a7] + UUIDs \n",
            "  [4ec0a83e] + Unicode \n",
            "  Building Conda ─→ `~/.julia/packages/Conda/3rPhK/deps/build.log`\n",
            "  Building IJulia → `~/.julia/packages/IJulia/DrVMH/deps/build.log`\n",
            "Precompiling project...\n",
            "Precompiling IJulia\n",
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4ufJv61foRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1d96e3b9-aeca-4ad9-ef04-7f9604c546d3"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add([\"Knet\",\"LinearAlgebra\", \"Random\", \"SparseArrays\", \"Statistics\",\"ScikitLearn\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.3/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.3/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ-z0F47V92U",
        "colab_type": "text"
      },
      "source": [
        "# Import Related Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtvetcUQV9N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using Knet, LinearAlgebra, Random, SparseArrays, Statistics, ScikitLearn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWmnYnoSWHB_",
        "colab_type": "text"
      },
      "source": [
        "Create network weights struct:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Re6Ul5hVoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9683558d-c769-4f09-a5ba-0394e49966b4"
      },
      "source": [
        "mutable struct network_weights\n",
        "    distance_parameter;\n",
        "    input_dim;\n",
        "    stride;\n",
        "    NpS;\n",
        "    output_dim;\n",
        "    W;\n",
        "    L;\n",
        "    W_structure;\n",
        "    L_structure;\n",
        "    previous_NpS;\n",
        "    lateral_distance;\n",
        "end\n",
        "    \n",
        "\n",
        "# defining struct constants for W matrix\n",
        "function create_h_distances(n::network_weights)\n",
        "    distances = zeros(n.output_dim^2, n.input_dim, n.input_dim)\n",
        "    dict_input_2_position = Dict()\n",
        "    for row_index in 1:n.input_dim\n",
        "        for column_index in 1:n.input_dim\n",
        "            input_index = row_index*n.input_dim + column_index\n",
        "            dict_input_2_position[row_index, column_index] = input_index\n",
        "        end\n",
        "    end\n",
        "    centers = []\n",
        "    dict_output_2_position = Dict()\n",
        "    for i in 1:n.output_dim\n",
        "        for j in 1:n.output_dim\n",
        "            stride_padding = n.stride/2\n",
        "            neuron_center = [i*n.stride + stride_padding, j*n.stride + stride_padding]\n",
        "            push!(centers,neuron_center)\n",
        "            neuron_index = (i-1)*n.output_dim + j\n",
        "            dict_output_2_position[neuron_index] = neuron_center\n",
        "            for k in 1:n.input_dim\n",
        "                 for l in 1:n.input_dim\n",
        "                    #size issue here neuron index becomes 50 where it should be 49\n",
        "                    distances[neuron_index, k,l] = norm([k+0.5,l+0.5]-neuron_center)\n",
        "                 end\n",
        "           end\n",
        "        end\n",
        "    end\n",
        "    above_threshold = distances .> n.distance_parameter\n",
        "    below_threshold = distances .<= n.distance_parameter\n",
        "    distances[above_threshold] .= 0\n",
        "    distances[below_threshold] .= 1\n",
        "    distances = reshape(distances,(n.output_dim^2, n.input_dim^2))\n",
        "    return distances\n",
        "end\n",
        "\n",
        "# defining structure constants for L matrix\n",
        "function create_ah_distances(n::network_weights)\n",
        "    centers = []\n",
        "    dict_output_2_position = Dict()\n",
        "    for i in 1:n.output_dim\n",
        "        for j in 1:n.output_dim\n",
        "            stride_padding = n.stride/2\n",
        "            neuron_center = [(i-1)*n.stride + stride_padding, (j-1)*n.stride + stride_padding]\n",
        "            push!(centers,neuron_center)\n",
        "            neuron_index = (i-1)*n.output_dim + j\n",
        "            dict_output_2_position[neuron_index] = neuron_center\n",
        "        end\n",
        "    end\n",
        "    distances_ah = zeros(n.output_dim^2, n.output_dim^2)\n",
        "    for row_index in keys(dict_output_2_position)\n",
        "        center = dict_output_2_position[row_index]\n",
        "        for column_index in keys(dict_output_2_position)\n",
        "            other_center = dict_output_2_position[column_index]\n",
        "            distances_ah[row_index, column_index] = norm(other_center - center)\n",
        "        end\n",
        "    end\n",
        "    above_threshold = distances_ah .> n.lateral_distance #*self.anti_hebbian_binary\n",
        "    below_threshold = distances_ah .<= n.lateral_distance #*self.anti_hebbian_binary\n",
        "    distances_ah[above_threshold] .= 0\n",
        "    distances_ah[below_threshold] .= 1\n",
        "    return distances_ah\n",
        "end    \n",
        "\n",
        "# Create lateral connections\n",
        "function create_L(n::network_weights)\n",
        "    mat = create_ah_distances(n)\n",
        "    L_mat = repeat(mat,n.NpS,n.NpS) ## !! Figure this one out\n",
        "    return L_mat\n",
        "end\n",
        "\n",
        "# Create feedforward connections \n",
        "function create_W(n::network_weights)\n",
        "    mat = create_h_distances(n)\n",
        "    W_mat = repeat(mat,n.NpS,n.previous_NpS) ## !! Figure this one out\n",
        "    return W_mat\n",
        "end\n",
        "    \n",
        "# Create weights matrices    \n",
        "function create_weights_matrix(n::network_weights)\n",
        "    n.W_structure = create_W(n)\n",
        "    n.L_structure = create_L(n)\n",
        "    factor = sqrt(((sum(n.W_structure)/n.NpS)/(n.output_dim^2)))\n",
        "    n.W = n.W_structure.*randn(size(n.W_structure))/factor\n",
        "    n.L = n.L_structure.*Matrix{Float64}(I,n.NpS * n.output_dim^2,n.NpS * n.output_dim^2)\n",
        "end\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "create_weights_matrix (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neeesBiCWYp1",
        "colab_type": "text"
      },
      "source": [
        "Activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yVu6TwjWXuA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a566897-62ca-4daa-ea1e-d8deec32eecc"
      },
      "source": [
        "# Activation function\n",
        "function activation_function(vec)\n",
        "    tanh.(vec)\n",
        "end"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "activation_function (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6rTO09-WacS",
        "colab_type": "text"
      },
      "source": [
        "GPU check and convert to GPU function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1prttJw2WVRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "349927b6-f5f2-47bf-cee6-21b5ccc655f3"
      },
      "source": [
        "#Check GPU availability\n",
        "println(gpu()>=0)\n",
        "if gpu() >= 0  # gpu() returns a device id >= 0 if there is a GPU, -1 otherwise \n",
        "    atype = KnetArray{Float32}  # KnetArrays are stored and operated in the GPU\n",
        "end\n",
        "# Converts arrays to Knet arrays to work on GPU\n",
        "function convert_to_GPU(vec)\n",
        "    vec=convert(atype,vec)\n",
        "end\n",
        "# Convert and floor to integer\n",
        "function int(x)\n",
        "convert(Int,floor(x))\n",
        "end\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyfFz5TsWiD_",
        "colab_type": "text"
      },
      "source": [
        "# Network object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UApZeu3oWQ0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49a6429b-ca90-425b-c78a-42bea0012f9e"
      },
      "source": [
        "# Network object\n",
        "mutable struct deep_network_GPU\n",
        "    image_dim;                  # input image dimensions\n",
        "    channels;                   # number of channels\n",
        "    NpSs;                       # number of neurons per site\n",
        "    strides;                    # stride array\n",
        "    distances;                  # feedforward connections distance threshold\n",
        "    lateral_distances;          # lateral connections distance threshold\n",
        "    layers;                     # number of layers\n",
        "    gamma;                      # feedback connections parameter\n",
        "    lr;                         # learning rate\n",
        "    lr_floor;                   # min learning rate\n",
        "    current_lr;                 # current learning rate\n",
        "    decay;                      # decay parameter of learning rate\n",
        "    conversion_tickers;         # checks if neural dynamics converged\n",
        "    epoch;                      # epoch counter\n",
        "    structure;          \n",
        "    deep_matrix_weights;        # Weights matrix\n",
        "    deep_matrix_structure;      # Structure matrix\n",
        "    deep_matrix_identity;       # \n",
        "    weights_adjustment_matrix;  #\n",
        "    weights_update_matrix;      #\n",
        "    n_images;                   #\n",
        "    dict_weights;               #\n",
        "    dimensions;                 #\n",
        "    euler_step;                 #\n",
        "    tanh_factors;               #\n",
        "    mult_factors;               #\n",
        "    W_gpu;                      #\n",
        "end\n",
        "\n",
        "# Create deep network\n",
        "function create_deep_network(dn::deep_network_GPU)\n",
        "    for i in 1:dn.layers+1\n",
        "        if i==1\n",
        "            dim=1\n",
        "        else\n",
        "            dim = int(prod(dn.strides[1:i-1]))\n",
        "        end\n",
        "        push!(dn.dimensions,(int((dn.image_dim/dim)^2)*hcat([dn.channels],dn.NpSs)[i]))\n",
        "    end\n",
        "\n",
        "    for i in 1:dn.layers\n",
        "        if i==1\n",
        "            dim=1\n",
        "        else\n",
        "            dim = int(prod(dn.strides[1:i-1]))\n",
        "        end\n",
        "        layer_input_dim = int(dn.image_dim/dim)\n",
        "\n",
        "         dn.dict_weights[i]=network_weights(dn.distances[i],layer_input_dim,dn.strides[i],hcat([dn.channels],dn.NpSs)[i+1],\n",
        "         int(layer_input_dim/dn.strides[i]),[],[],\n",
        "         [], [], hcat([dn.channels],dn.NpSs)[i], dn.lateral_distances[i])\n",
        "         create_weights_matrix(dn.dict_weights[i])\n",
        "\n",
        "    end\n",
        "\n",
        "    matrix_block = []\n",
        "    structure_block = []\n",
        "    matrix_identity = []\n",
        "    weight_adjustment_block = []\n",
        "    gradient_update_block = []\n",
        "    abs_structure_block = []\n",
        "    for (i, ele_row) in enumerate(dn.dimensions)\n",
        "        #println(\"inside the loop\")\n",
        "        #println(i)\n",
        "        #println(ele_row)\n",
        "        row_block = []\n",
        "        struc_block = []\n",
        "        row_identity_block = []\n",
        "        weights_adj_block = []\n",
        "        grad_update_block = []\n",
        "        abs_struc_block = []\n",
        "        start_block = max(i-1, 1)\n",
        "\n",
        "        end_block = max(length(dn.dimensions)-start_block-2, 1)\n",
        "        if i == 1\n",
        "            #println(\"pushed 1 element to row_block\")\n",
        "            push!(row_block,zeros(ele_row, sum(dn.dimensions)))\n",
        "            push!(struc_block,zeros(ele_row, sum(dn.dimensions)))\n",
        "            push!(row_identity_block, zeros(ele_row, sum(dn.dimensions)))\n",
        "            push!(weights_adj_block, zeros(ele_row, sum(dn.dimensions)))\n",
        "            push!(grad_update_block, zeros((ele_row, sum(dn.dimensions))))\n",
        "            push!(abs_struc_block, zeros((ele_row, sum(dn.dimensions))))\n",
        "        elseif i == length(dn.dimensions)\n",
        "            #i=2 here\n",
        "            if start_block > 1\n",
        "                push!(row_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                push!(struc_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                push!(row_identity_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                push!(weights_adj_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                push!(grad_update_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                push!(abs_struc_block,zeros(ele_row, int(sum(dn.dimensions[1:start_block]))))\n",
        "                #println(\"second elseif first if\")\n",
        "                #println(size(zeros(ele_row, int(sum(dn.dimensions[1:start_block])))))\n",
        "            end\n",
        "            #println(\"pushed 1 element to row_block\")\n",
        "            push!(row_block,dn.dict_weights[i-1].W)\n",
        "            #println(\"pushed 1 element to row_block\")\n",
        "            push!(row_block,dn.dict_weights[i-1].L)\n",
        "            push!(struc_block,dn.dict_weights[i-1].W_structure/dn.mult_factors[i-1])\n",
        "            push!(struc_block,-dn.dict_weights[i-1].L_structure)\n",
        "\n",
        "            push!(abs_struc_block,dn.dict_weights[i-1].W_structure)\n",
        "            push!(abs_struc_block,dn.dict_weights[i-1].L_structure)\n",
        "\n",
        "            push!(row_identity_block,zeros(size(dn.dict_weights[i-1].W_structure)))\n",
        "            push!(row_identity_block,Matrix{Float64}(I,size(dn.dict_weights[i-1].L_structure,1),size(dn.dict_weights[i-1].L_structure,1))) #identity matrix\n",
        "\n",
        "            push!(weights_adj_block,dn.dict_weights[i-1].W_structure)\n",
        "            push!(weights_adj_block,dn.dict_weights[i-1].L_structure)\n",
        "\n",
        "            push!(grad_update_block,dn.dict_weights[i-1].W_structure)\n",
        "            push!(grad_update_block,dn.dict_weights[i-1].L_structure/2)\n",
        "\n",
        "        end\n",
        "        # we only push row_block to matrix_block. Check row_block\n",
        "        push!(matrix_block,row_block)\n",
        "        push!(structure_block,struc_block)\n",
        "        push!(matrix_identity,row_identity_block)\n",
        "        push!(weight_adjustment_block,weights_adj_block)\n",
        "        push!(gradient_update_block,grad_update_block)\n",
        "        push!(abs_structure_block,abs_struc_block)\n",
        "    end\n",
        "\n",
        "\n",
        "    dn.deep_matrix_weights = convert_to_GPU(vcat(matrix_block[1][1],hcat(matrix_block[2][1],matrix_block[2][2]))) \n",
        "    dn.deep_matrix_structure = convert_to_GPU(vcat(structure_block[1][1],hcat(structure_block[2][1],structure_block[2][2])))\n",
        "    dn.deep_matrix_identity = convert_to_GPU(vcat(matrix_identity[1][1],hcat(matrix_identity[2][1],matrix_identity[2][2])))\n",
        "    dn.weights_adjustment_matrix = convert_to_GPU(vcat(weight_adjustment_block[1][1],hcat(weight_adjustment_block[2][1],weight_adjustment_block[2][2])))\n",
        "    dn.weights_update_matrix = convert_to_GPU(vcat(gradient_update_block[1][1],hcat(gradient_update_block[2][1],gradient_update_block[2][2])))\n",
        "    dn.structure = convert_to_GPU(vcat(abs_structure_block[1][1],hcat(abs_structure_block[2][1],abs_structure_block[2][2])))\n",
        "\n",
        "    println(\"deep_network created\")\n",
        "end\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "create_deep_network (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap3QYiaoXfeI",
        "colab_type": "text"
      },
      "source": [
        "# Neural Dynamics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJ464mkWr55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d48619a1-79de-4ccf-c029-22c471c78b67"
      },
      "source": [
        "# First define the neural dynamics\n",
        "function neural_dynamics(dn::deep_network_GPU,img)\n",
        "    conversion_ticker = 0\n",
        "    x = img\n",
        "    u_vec = convert_to_GPU(zeros(sum(dn.dimensions)))\n",
        "    #representation vector\n",
        "    r_vec = convert_to_GPU(zeros(sum(dn.dimensions)))\n",
        "    \n",
        "    r_vec[1:dn.channels*dn.image_dim^2] .= x\n",
        "    delta = repeat([1e10],dn.layers) \n",
        "\n",
        "    dn.W_gpu = dn.deep_matrix_weights.*dn.deep_matrix_structure .+ dn.deep_matrix_identity\n",
        "    updates = 0\n",
        "    while updates < 3000\n",
        "        if sum(delta .< 1e-4) == length(delta)\n",
        "            conversion_ticker=1\n",
        "            break\n",
        "        end\n",
        "        lr = max((dn.euler_step/(1+0.005*updates)), 0.05)\n",
        "        delta_u = -u_vec .+ dn.W_gpu*r_vec\n",
        "        u_vec[dn.channels*(dn.image_dim)^2:end] += lr*delta_u[dn.channels*dn.image_dim^2:end]\n",
        "        r_vec[dn.channels*(dn.image_dim)^2:end] = activation_function(u_vec[dn.channels*dn.image_dim^2:end])\n",
        "        updates += 1\n",
        "        if (updates+1)%100 == 0\n",
        "            # may be problems with indexing here. made changes here that may not work under different modes.\n",
        "            #println(size(dn.dimensions[2:end]))\n",
        "            for layer in 1:dn.layers\n",
        "                start_token_large = sum(dn.dimensions[1:layer])\n",
        "                end_token_large = sum(dn.dimensions[1:layer+1])\n",
        "                start_token_small = int(sum(dn.dimensions[2:end])) \n",
        "                end_token_small = sum(dn.dimensions[2:end]) \n",
        "                delta_layer = norm(delta_u[start_token_small:end_token_small])/norm(u_vec[start_token_large:end_token_large])\n",
        "                delta[layer] = delta_layer\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return r_vec, conversion_ticker\n",
        "end\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neural_dynamics (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FV0mmo9XwTN",
        "colab_type": "text"
      },
      "source": [
        "# Weight Updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vUzKQfXWubF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdb00d31-65d8-4523-f234-65ff0fcfa64a"
      },
      "source": [
        "\n",
        "# Update weights via stochastic gradient ascent-descent\n",
        "function update_weights(dn::deep_network_GPU,r_vec)\n",
        "     dn.current_lr = max(dn.lr/(1+dn.decay*dn.epoch), dn.lr_floor)\n",
        "     update_matrix = r_vec*r_vec' #update for L matrix\n",
        "     grad_weights = convert_to_GPU(dn.weights_update_matrix).*(update_matrix - convert_to_GPU(dn.weights_adjustment_matrix.*dn.deep_matrix_weights))\n",
        "     dn.deep_matrix_weights += dn.current_lr*grad_weights\n",
        "end\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "update_weights (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlMVQDWoXzwg",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NdVE07bWwAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d6318b0-e887-4f90-d256-dc18ae469fa6"
      },
      "source": [
        "# training function to call neural dynamics and gradient ascent-descent \n",
        "function training(dn::deep_network_GPU, images)\n",
        "     dn.n_images = size(images,1)\n",
        "         sum_ticker = 0\n",
        "         for img in 1:size(images,1)\n",
        "             r, conversion_ticker = neural_dynamics(dn,images[img,:])\n",
        "             sum_ticker += conversion_ticker\n",
        "             update_weights(dn,r)\n",
        "         end\n",
        "         dn.epoch+=1\n",
        "         push!(dn.conversion_tickers,sum_ticker/dn.n_images)\n",
        "         \n",
        "         # Commented these out for speed\n",
        "    \n",
        "         println(\"\")\n",
        "         println(\"Epoch: \"*string(dn.epoch))\n",
        "         println(\"Conversion: \"*string(dn.conversion_tickers[end]))\n",
        "         println(\"Current Learning Rate: \"*string(dn.current_lr)) \n",
        "         println(\"\")\n",
        " end        "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "training (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-cKkkyFX10n",
        "colab_type": "text"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7UVvgzOf0vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81f1dce2-30ed-48ee-a39c-6a65652fe43a"
      },
      "source": [
        "# import mnist and preprocess\n",
        "\n",
        "function get_mnist()\n",
        "    \n",
        "    #dtrn,dtst = mnistdata(batchsize=1;xsize=(784,:),xtype=atype) # The setting is online so batchsize is set to 1.\n",
        "    X_train, y_train, X_test, y_test = mnist() # loading the data\n",
        "    X_train = convert(atype,reshape(X_train, 784, 60000)')\n",
        "    X_test = convert(atype,reshape(X_test, 784, 10000)')\n",
        "    \n",
        "     # preprocess data\n",
        "     X_train =X_train .- mean(X_train;dims=2)\n",
        "     X_test =X_test.- mean(X_test;dims=2)\n",
        "    \n",
        "    #summary.(first(X_train))\n",
        "    return X_train, X_test, y_train, y_test\n",
        "end\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "get_mnist (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS3S-w2X5bX",
        "colab_type": "text"
      },
      "source": [
        "# Main function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChcEJOwphffv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0858f61-605f-4ad2-b6c9-a9b6c0801149"
      },
      "source": [
        "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data\n",
        "x_train, x_test, y_train, y_test = get_mnist()\n",
        "\n",
        "\n",
        "# network parameters\n",
        "NpSs=[4]\n",
        "image_dim = 28\n",
        "channels = 1\n",
        "strides = [2]\n",
        "distances = [4]\n",
        "tanh_factors = 1 \n",
        "layers = length(distances)\n",
        "distances_lateral =  repeat([0],layers)\n",
        "gamma = 0\n",
        "mult_factors = 1\n",
        "\n",
        "lr=5e-3;\n",
        "lr_floor = 1e-4;\n",
        "decay=0.5\n",
        "conversion_tickers = []\n",
        "epoch =0\n",
        "structure =[]\n",
        "deep_matrix_weights= [];\n",
        "deep_matrix_structure = [];\n",
        "deep_matrix_identity= [] ;\n",
        "weights_adjustment_matrix= [];\n",
        "weights_update_matrix= [];\n",
        "grad_matrix =[];\n",
        "n_images =[];\n",
        "dict_weights = Dict()\n",
        "dimensions = [];\n",
        "euler_step=0.2;\n",
        "tanh_factors=1;\n",
        "mult_factors=1;\n",
        "W_gpu=[];\n",
        "\n",
        "println(\"initializing the network\")\n",
        "\n",
        "    network= deep_network_GPU(image_dim,channels,NpSs,strides,distances,distances_lateral, layers,gamma, lr,lr_floor,\n",
        "                            lr, decay,conversion_tickers, epoch, structure, deep_matrix_weights,\n",
        "                            deep_matrix_structure, deep_matrix_identity, weights_adjustment_matrix,\n",
        "                            weights_update_matrix, n_images, dict_weights, dimensions,\n",
        "                            euler_step, tanh_factors,mult_factors , W_gpu);\n",
        "    \n",
        "    println(\"network defined\")\n",
        "\n",
        "    create_deep_network(network)\n",
        "    \n",
        "    println(\"NpS: \"*string(network.NpSs))\n",
        "    println(\"Strides: \"*string(network.strides))\n",
        "    println(\"Distances: \"*string(network.distances))\n",
        "    println(\"Lateral Distances: \"*string(network.lateral_distances))\n",
        "    println(\"gamma : \"*string(network.gamma))\n",
        "    println(\"tanh_factor : \"*string(network.tanh_factors))\n",
        "    \n",
        "    no_of_epochs=120\n",
        "    for i in 1:no_of_epochs\n",
        "        indices = 1:size(x_train,1)\n",
        "        rand_indices = rand(indices, 1000)\n",
        "        x_train_rand = x_train[rand_indices,:]\n",
        "        training(network, x_train_rand)\n",
        "    end\n",
        "    x_train_rand=nothing\n",
        "\n",
        "println(\"DONE\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initializing the network\n",
            "network defined\n",
            "deep_network created\n",
            "NpS: [4]\n",
            "Strides: [2]\n",
            "Distances: [4]\n",
            "Lateral Distances: [0]\n",
            "gamma : 0\n",
            "tanh_factor : 1\n",
            "\n",
            "Epoch: 1\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.005\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0033333333333333335\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0025\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.002\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0016666666666666668\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0014285714285714286\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00125\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0011111111111111111\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.001\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0009090909090909091\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0008333333333333334\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0007692307692307692\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0007142857142857143\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0006666666666666666\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.000625\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0005882352941176471\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0005555555555555556\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0005263157894736842\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0005\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0004761904761904762\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00045454545454545455\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0004347826086956522\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0004166666666666667\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0004\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0003846153846153846\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00037037037037037035\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00035714285714285714\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0003448275862068966\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0003333333333333333\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0003225806451612903\n",
            "\n",
            "\n",
            "Epoch: 31\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0003125\n",
            "\n",
            "\n",
            "Epoch: 32\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00030303030303030303\n",
            "\n",
            "\n",
            "Epoch: 33\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00029411764705882356\n",
            "\n",
            "\n",
            "Epoch: 34\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00028571428571428574\n",
            "\n",
            "\n",
            "Epoch: 35\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002777777777777778\n",
            "\n",
            "\n",
            "Epoch: 36\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002702702702702703\n",
            "\n",
            "\n",
            "Epoch: 37\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002631578947368421\n",
            "\n",
            "\n",
            "Epoch: 38\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002564102564102564\n",
            "\n",
            "\n",
            "Epoch: 39\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00025\n",
            "\n",
            "\n",
            "Epoch: 40\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00024390243902439024\n",
            "\n",
            "\n",
            "Epoch: 41\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002380952380952381\n",
            "\n",
            "\n",
            "Epoch: 42\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00023255813953488373\n",
            "\n",
            "\n",
            "Epoch: 43\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00022727272727272727\n",
            "\n",
            "\n",
            "Epoch: 44\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00022222222222222223\n",
            "\n",
            "\n",
            "Epoch: 45\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002173913043478261\n",
            "\n",
            "\n",
            "Epoch: 46\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002127659574468085\n",
            "\n",
            "\n",
            "Epoch: 47\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00020833333333333335\n",
            "\n",
            "\n",
            "Epoch: 48\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00020408163265306123\n",
            "\n",
            "\n",
            "Epoch: 49\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0002\n",
            "\n",
            "\n",
            "Epoch: 50\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.000196078431372549\n",
            "\n",
            "\n",
            "Epoch: 51\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001923076923076923\n",
            "\n",
            "\n",
            "Epoch: 52\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00018867924528301886\n",
            "\n",
            "\n",
            "Epoch: 53\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00018518518518518518\n",
            "\n",
            "\n",
            "Epoch: 54\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00018181818181818183\n",
            "\n",
            "\n",
            "Epoch: 55\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00017857142857142857\n",
            "\n",
            "\n",
            "Epoch: 56\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00017543859649122808\n",
            "\n",
            "\n",
            "Epoch: 57\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001724137931034483\n",
            "\n",
            "\n",
            "Epoch: 58\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00016949152542372882\n",
            "\n",
            "\n",
            "Epoch: 59\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00016666666666666666\n",
            "\n",
            "\n",
            "Epoch: 60\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001639344262295082\n",
            "\n",
            "\n",
            "Epoch: 61\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00016129032258064516\n",
            "\n",
            "\n",
            "Epoch: 62\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00015873015873015873\n",
            "\n",
            "\n",
            "Epoch: 63\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00015625\n",
            "\n",
            "\n",
            "Epoch: 64\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00015384615384615385\n",
            "\n",
            "\n",
            "Epoch: 65\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00015151515151515152\n",
            "\n",
            "\n",
            "Epoch: 66\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00014925373134328358\n",
            "\n",
            "\n",
            "Epoch: 67\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00014705882352941178\n",
            "\n",
            "\n",
            "Epoch: 68\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00014492753623188405\n",
            "\n",
            "\n",
            "Epoch: 69\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00014285714285714287\n",
            "\n",
            "\n",
            "Epoch: 70\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00014084507042253522\n",
            "\n",
            "\n",
            "Epoch: 71\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001388888888888889\n",
            "\n",
            "\n",
            "Epoch: 72\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.000136986301369863\n",
            "\n",
            "\n",
            "Epoch: 73\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00013513513513513514\n",
            "\n",
            "\n",
            "Epoch: 74\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00013333333333333334\n",
            "\n",
            "\n",
            "Epoch: 75\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00013157894736842105\n",
            "\n",
            "\n",
            "Epoch: 76\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00012987012987012987\n",
            "\n",
            "\n",
            "Epoch: 77\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001282051282051282\n",
            "\n",
            "\n",
            "Epoch: 78\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00012658227848101267\n",
            "\n",
            "\n",
            "Epoch: 79\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.000125\n",
            "\n",
            "\n",
            "Epoch: 80\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001234567901234568\n",
            "\n",
            "\n",
            "Epoch: 81\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00012195121951219512\n",
            "\n",
            "\n",
            "Epoch: 82\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00012048192771084338\n",
            "\n",
            "\n",
            "Epoch: 83\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011904761904761905\n",
            "\n",
            "\n",
            "Epoch: 84\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011764705882352942\n",
            "\n",
            "\n",
            "Epoch: 85\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011627906976744187\n",
            "\n",
            "\n",
            "Epoch: 86\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011494252873563218\n",
            "\n",
            "\n",
            "Epoch: 87\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011363636363636364\n",
            "\n",
            "\n",
            "Epoch: 88\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011235955056179776\n",
            "\n",
            "\n",
            "Epoch: 89\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00011111111111111112\n",
            "\n",
            "\n",
            "Epoch: 90\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010989010989010989\n",
            "\n",
            "\n",
            "Epoch: 91\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010869565217391305\n",
            "\n",
            "\n",
            "Epoch: 92\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010752688172043011\n",
            "\n",
            "\n",
            "Epoch: 93\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010638297872340425\n",
            "\n",
            "\n",
            "Epoch: 94\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010526315789473685\n",
            "\n",
            "\n",
            "Epoch: 95\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010416666666666667\n",
            "\n",
            "\n",
            "Epoch: 96\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010309278350515464\n",
            "\n",
            "\n",
            "Epoch: 97\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010204081632653062\n",
            "\n",
            "\n",
            "Epoch: 98\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.00010101010101010101\n",
            "\n",
            "\n",
            "Epoch: 99\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 100\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 101\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 102\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 103\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 104\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 105\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 106\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 107\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 108\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 109\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 110\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 111\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 112\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 113\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 114\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 115\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 116\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 117\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 118\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 119\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "\n",
            "Epoch: 120\n",
            "Conversion: 1.0\n",
            "Current Learning Rate: 0.0001\n",
            "\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OqJC8cX8Nh",
        "colab_type": "text"
      },
      "source": [
        "# Learn Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD1uxWrHBYuA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "156a4c95-8fc8-451a-9510-76b9e4035751"
      },
      "source": [
        "\n",
        "# learn representations for tranining data\n",
        "train_representations=[]\n",
        "#for i in  1:int(size(x_train,1))\n",
        "N_m=60000\n",
        "for i in 1:N_m\n",
        "    train_rep, _ = neural_dynamics(network,x_train[i,:])\n",
        "     push!(train_representations, train_rep)\n",
        " end\n",
        "\n",
        "# clear train data from memory\n",
        "x_train=nothing\n",
        "Knet.gc()\n",
        "\n",
        "# this conversion is needed for ScikitLearn\n",
        "for i in 1:length(train_representations)\n",
        "    train_representations[i]=convert(Array{Float64,1},train_representations[i])\n",
        "end\n",
        "train_representations=hcat(train_representations...)'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000×1568 Adjoint{Float64,Array{Float64,2}}:\n",
              " -0.13768    -0.13768    -0.13768    …  -0.352322  -0.338648  0.238573\n",
              " -0.155537   -0.155537   -0.155537      -0.390651  -0.374094  0.264187\n",
              " -0.0972539  -0.0972539  -0.0972539     -0.257321  -0.249663  0.175386\n",
              " -0.0857093  -0.0857093  -0.0857093     -0.228335  -0.22216   0.156062\n",
              " -0.116116   -0.116116   -0.116116      -0.303     -0.292663  0.205758\n",
              " -0.148064   -0.148064   -0.148064   …  -0.374893  -0.35955   0.253639\n",
              " -0.0882653  -0.0882653  -0.0882653     -0.234814  -0.228322  0.160386\n",
              " -0.179407   -0.179407   -0.179407      -0.438264  -0.417826  0.296261\n",
              " -0.0543918  -0.0543918  -0.0543918     -0.146701  -0.143776  0.101142\n",
              " -0.109564   -0.109564   -0.109564      -0.287385  -0.27801   0.195381\n",
              " -0.142797   -0.142797   -0.142797   …  -0.363542  -0.349049  0.246057\n",
              " -0.0712785  -0.0712785  -0.0712785     -0.191184  -0.186658  0.131179\n",
              " -0.178877   -0.178877   -0.178877      -0.437251  -0.416899  0.295575\n",
              "  ⋮                                  ⋱   ⋮                            \n",
              " -0.130007   -0.130007   -0.130007      -0.335145  -0.322683  0.227133\n",
              " -0.113245   -0.113245   -0.113245      -0.296192  -0.286281  0.201234\n",
              " -0.0920018  -0.0920018  -0.0920018  …  -0.244223  -0.237256  0.166661\n",
              " -0.109169   -0.109169   -0.109169      -0.286434  -0.277116  0.194749\n",
              " -0.0808123  -0.0808123  -0.0808123     -0.215833  -0.210245  0.147706\n",
              " -0.0653912  -0.0653912  -0.0653912     -0.175781  -0.171856  0.120812\n",
              " -0.0481543  -0.0481543  -0.0481543     -0.130067  -0.127639  0.089829\n",
              " -0.129062   -0.129062   -0.129062   …  -0.332999  -0.320685  0.225705\n",
              " -0.13648    -0.13648    -0.13648       -0.349662  -0.336179  0.2368  \n",
              " -0.110704   -0.110704   -0.110704      -0.290123  -0.280583  0.1972  \n",
              " -0.102181   -0.102181   -0.102181      -0.269464  -0.261134  0.183466\n",
              " -0.104642   -0.104642   -0.104642      -0.275475  -0.266802  0.187463"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBiE1AdtBb3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "a94c4bf7-a678-48a1-dc72-6cb0153520ea"
      },
      "source": [
        "# learn representations for test data\n",
        "test_representations = []\n",
        "for i in 1:size(x_test,1)\n",
        "#for i in 1:N_m\n",
        "      test_rep, _ = neural_dynamics(network, x_test[i,:])\n",
        "      push!(test_representations,test_rep)\n",
        "  end\n",
        "\n",
        "# clear test data from memory\n",
        "x_test=nothing\n",
        "Knet.gc()\n",
        "\n",
        "for i in 1:length(test_representations)\n",
        "    test_representations[i]=convert(Array{Float64,1},test_representations[i])\n",
        "end\n",
        "\n",
        "test_representations=hcat(test_representations...)'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000×1568 Adjoint{Float64,Array{Float64,2}}:\n",
              " -0.0923069  -0.0923069  -0.0923069  …  -0.244988  -0.237982  0.167171 \n",
              " -0.144308   -0.144308   -0.144308      -0.258814  -0.352082  0.248244 \n",
              " -0.0493748  -0.0493748  -0.0493748     -0.133329  -0.130808  0.0920509\n",
              " -0.185144   -0.185144   -0.185144      -0.449099  -0.42774   0.303614 \n",
              " -0.0962235  -0.0962235  -0.0962235     -0.254763  -0.247243  0.173683 \n",
              " -0.0693027  -0.0693027  -0.0693027  …  -0.186029  -0.181709  0.127713 \n",
              " -0.105962   -0.105962   -0.105962      -0.278685  -0.269826  0.189598 \n",
              " -0.105352   -0.105352   -0.105352      -0.277203  -0.26843   0.188612 \n",
              " -0.153731   -0.153731   -0.153731      -0.386881  -0.370617  0.261661 \n",
              " -0.156813   -0.156813   -0.156813      -0.3933    -0.376535  0.265963 \n",
              " -0.148865   -0.148865   -0.148865   …  -0.3766    -0.361127  0.25478  \n",
              " -0.13873    -0.13873    -0.13873       -0.354641  -0.340799  0.240119 \n",
              " -0.114461   -0.114461   -0.114461      -0.299081  -0.28899   0.203154 \n",
              "  ⋮                                  ⋱   ⋮                             \n",
              " -0.154862   -0.154862   -0.154862      -0.389244  -0.372797  0.263244 \n",
              " -0.187415   -0.187415   -0.187415      -0.453323  -0.431603  0.306487 \n",
              " -0.132643   -0.132643   -0.132643   …  -0.341093  -0.328217  0.231093 \n",
              " -0.226761   -0.226761   -0.226761      -0.521001  -0.493306  0.353084 \n",
              " -0.14925    -0.14925    -0.14925       -0.37742   -0.361885  0.255329 \n",
              " -0.2742     -0.2742     -0.2742        -0.590066  -0.556232  0.402122 \n",
              " -0.0862295  -0.0862295  -0.0862295     -0.229656  -0.223418  0.156944 \n",
              " -0.193943   -0.193943   -0.193943   …  -0.465267  -0.442514  0.314629 \n",
              " -0.172149   -0.172149   -0.172149      -0.424222  -0.404959  0.286765 \n",
              " -0.146213   -0.146213   -0.146213      -0.370928  -0.355884  0.250989 \n",
              " -0.132848   -0.132848   -0.132848      -0.341554  -0.328646  0.231399 \n",
              " -0.209249   -0.209249   -0.209249      -0.492137  -0.46702   0.333066 "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bwbMRUQYAiO",
        "colab_type": "text"
      },
      "source": [
        "# Clasify Using Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs9fu0qNXOE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6aab0343-f29c-421f-babb-3c944a2b73b8"
      },
      "source": [
        "classifier = SVC(max_iter=1e6, class_weight=\"balanced\",tol=1e-5,random_state=0,kernel=\"linear\")\n",
        "\n",
        "# fit linear SVM to learned train representations\n",
        "\n",
        "fit!(classifier, train_representations[:,end-network.dimensions[end]+1:end], y_train[1:N_m])\n",
        "\n",
        "train_score = score(classifier, train_representations[:,end-network.dimensions[end]+1:end], y_train[1:N_m])\n",
        "println(\"train_score\")\n",
        "println(train_score)\n",
        "println(\"train_error\")\n",
        "println(1-train_score)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_score\n",
            "0.9922166666666666\n",
            "train_error\n",
            "0.007783333333333364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wShc-zMYEAg",
        "colab_type": "text"
      },
      "source": [
        "# Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOAbCv40f0rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "00167c78-d70d-4831-83bf-437e0a1c0e7a"
      },
      "source": [
        "# classify the test representations on the trained linear SVM classifier\n",
        "test_score=score(classifier,test_representations[:,end-network.dimensions[end]+1:end], y_test)\n",
        "\n",
        "println(\"test_score\")\n",
        "println(test_score)\n",
        "\n",
        "println(\"test_error\")\n",
        "println(1-test_score)\n",
        "\n",
        "# print number of data points used to train\n",
        "println(\"\")\n",
        "println(\"Number of data points used to train the network\")\n",
        "println(N_m)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_score\n",
            "0.9679\n",
            "test_error\n",
            "0.03210000000000002\n",
            "\n",
            "Number of data points used to train the network\n",
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scbPmC1cYaIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}